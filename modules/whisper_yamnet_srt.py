import whisper
import datetime
import os
import librosa
import torch
import numpy as np
import pandas as pd
from panns_inference import AudioTagging

# PANNs Cnn14 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (torch hub)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
panns_model = AudioTagging(checkpoint_path=None)
panns_model.to(device)
panns_model.eval()

# PANNs AudioSet í´ë˜ìŠ¤ ì´ë¦„ (ì¶œì²˜: https://github.com/qiuqiangkong/audioset_tagging_cnn)
# ì¤‘ìš”í•œ 'Laughter'ì™€ 'Screaming' í¬í•¨ë¨
panns_class_map = [
    "Speech", "Music", "Laughter", "Screaming",
    # ... í•„ìš”ì‹œ ì „ì²´ 527ê°œ í´ë˜ìŠ¤ ë„£ì„ ìˆ˜ ìˆìŒ
]

def format_srt_timestamp(seconds: float) -> str:
    td = datetime.timedelta(seconds=seconds)
    total_seconds = int(td.total_seconds())
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60
    milliseconds = int(td.microseconds / 1000)
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"

def detect_laughter_scream(wav, sr, threshold=0.2):
    # PANNsëŠ” 32000Hz ì…ë ¥
    if sr != 32000:
        wav = librosa.resample(wav, orig_sr=sr, target_sr=32000)
        sr = 32000
    
    # ëª¨ë¸ ì…ë ¥ìš© ë³€í™˜
    wav = torch.tensor(wav, dtype=torch.float32).to(device)
    if wav.dim() == 1:
        wav = wav.unsqueeze(0)  # (1, samples)
    
    with torch.no_grad():
        output_dict = panns_model(wav)
    scores = output_dict['clipwise_output'].cpu().numpy()[0]  # (527,)

    labels = []
    # 'Laughter'ëŠ” í´ë˜ìŠ¤ ì¸ë±ìŠ¤ 2, 'Screaming'ì€ 3 (ìœ„ ë°°ì—´ ê¸°ì¤€)
    if scores[2] > threshold:
        labels.append("[ì›ƒìŒ]")
    if scores[3] > threshold:
        labels.append("[ë¹„ëª…]")
    return " ".join(labels)

def write_srt(segments, audio_path, file, threshold=0.2):
    y, sr = librosa.load(audio_path, sr=None)

    for i, segment in enumerate(segments, start=1):
        start_sample = int(segment['start'] * sr)
        end_sample = int(segment['end'] * sr)
        clip = y[start_sample:end_sample]

        tags = detect_laughter_scream(clip, sr, threshold=threshold)
        text = segment['text'].strip() + (" " + tags if tags else "")

        start = format_srt_timestamp(segment['start'])
        end = format_srt_timestamp(segment['end'])

        file.write(f"{i}\n")
        file.write(f"{start} --> {end}\n")
        file.write(f"{text}\n\n")

def transcribe_to_srt_with_labels(audio_path, model_size="base", threshold=0.2):
    model = whisper.load_model(model_size)
    print("ğŸ¤ Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

    result = model.transcribe(audio_path, verbose=True)

    base_name = os.path.splitext(audio_path)[0]
    srt_path = base_name + "_tagged.srt"

    with open(srt_path, "w", encoding="utf-8") as srt_file:
        write_srt(result["segments"], audio_path, srt_file, threshold=threshold)

    print(f"âœ… ë¼ë²¨ ì¶”ê°€ëœ SRT ì €ì¥ ì™„ë£Œ: {srt_path}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Whisper + PANNs(Cnn14)ìœ¼ë¡œ ë¼ë²¨ í¬í•¨ SRT ìë§‰ ìƒì„±ê¸°")
    parser.add_argument("audio_file", help="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (MP3/WAV ë“±)")
    parser.add_argument("--model", default="base", help="Whisper ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)")
    parser.add_argument("--threshold", type=float, default=0.2, help="PANNs ê°ì§€ ì‹ ë¢°ë„ ê¸°ì¤€ (0~1 ì‚¬ì´)")
    args = parser.parse_args()

    transcribe_to_srt_with_labels(args.audio_file, model_size=args.model, threshold=args.threshold)